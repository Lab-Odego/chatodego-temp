{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "Or3nnUF0xlTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MPCxX6SalwIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026316ca-e6a9-41ff-9282-9c645a3aa8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/jinwoo\n"
          ]
        }
      ],
      "source": [
        "# mount at Google Drive & Change directory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive/jinwoo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install library: transformers, import_ipynb, openai\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install import_ipynb\n",
        "!pip install openai\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "jw8k4Lv2l31v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854d8d61-ab34-467a-c4e9-1d56ddb0a4f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-zthj_f0q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-zthj_f0q\n",
            "  Resolved https://github.com/huggingface/transformers to commit 888c4a2ae02fb4e6c4b53a4e6eb0b973a771362f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (0.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.29.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.29.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.29.0.dev0) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.9/dist-packages (from import_ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from import_ipynb) (5.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (67.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (3.0.38)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (2.14.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (0.18.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (0.1.6)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->import_ipynb) (2.16.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.9/dist-packages (from nbformat->import_ipynb) (5.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core->nbformat->import_ipynb) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.9/dist-packages (0.38.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library & Declare Constant"
      ],
      "metadata": {
        "id": "B-CCfjCmxtSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fa-8-KxJkgQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c5a0ba-8a68-42a7-f39e-e0cfc48e4eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from utils.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Drive Directory that downloaded pre-trained models - /content/drive/MyDrive/jinwoo/huggingface/...\n",
        "import transformers\n",
        "from transformers import Trainer, GenerationConfig\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "\n",
        "import import_ipynb\n",
        "import utils\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "ROOT_PATH = \"/content/drive/MyDrive/jinwoo\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/jinwoo/model\"\n",
        "DATA_PATH = \"/content/drive/MyDrive/jinwoo/dataset/ko_alpaca_data.json\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/jinwoo/result\"\n",
        "\n",
        "MODEL_NAME = \"decapoda-research/llama-7b-hf\"\n",
        "OUTPUT_DIR = ROOT_PATH\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    # 추가적인 input data가 있는 경우,\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    # 추가적인 input data가 없는 경우,\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arguments Class"
      ],
      "metadata": {
        "id": "VJXHMarQx62l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "enj8dSLRk6jD"
      },
      "outputs": [],
      "source": [
        "# Parser Arguments - Path of model\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
        "\n",
        "# Parser Arguments - Path of dataset\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
        "\n",
        "# Parser Arguments - Using args for training\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    \"\"\"    Input Arguments using for training.    \"\"\"\n",
        "    \n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    num_train_epochs: float = field(default=1)\n",
        "    save_strategy: str = field(default='epoch')\n",
        "    dataloader_num_workers: int = field(default=8)\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    auto_find_batch_size: bool = field(default=True)\n",
        "    model_max_length: int = field(\n",
        "        default=512, \n",
        "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Functions"
      ],
      "metadata": {
        "id": "AJz5MgGhyDYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1jYqioMNlRiS"
      },
      "outputs": [],
      "source": [
        "# Save - model's states\n",
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"    Collects the state dict and dump to disk.    \"\"\"\n",
        "    # 모델의 weight를 dictionary 형태로 저장\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        # weight들이 cuda에 있으므로, cpu에 위치하게 바꿈.\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        # 가중치를 저장함.\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zokV--rAlYRw"
      },
      "outputs": [],
      "source": [
        "# Resize - Tokenizer & Embedding\n",
        "def smart_tokenizer_and_embedding_resize(special_tokens_dict: Dict, tokenizer: transformers.PreTrainedTokenizer, \n",
        "                                         model: transformers.PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Resize tokenizer and embedding.\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DA5_ZQRhlbcu"
      },
      "outputs": [],
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"    Tokenize a list of strings.    \"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KvG0MWXGleeN"
      },
      "outputs": [],
      "source": [
        "def preprocess(sources: Sequence[str], targets: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"    Preprocess the data by tokenizing.    \"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Class & Function"
      ],
      "metadata": {
        "id": "TeOGOMaFyIj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "n4BUyYILlhPJ"
      },
      "outputs": [],
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"    Dataset for supervised fine-tuning.    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        # alpaca_data.json 파일에서 list 형태로 데이터 읽음.\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = utils.jload(data_path)\n",
        "\n",
        "        # prompt 생성\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "\n",
        "        #\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5BvZHpP1ljco"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"    Collate examples for supervised fine-tuning.    \"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GIthVOmeljqU"
      },
      "outputs": [],
      "source": [
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
        "    \"\"\"    Make dataset and collator for supervised fine-tuning.    \"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model & Tokenizer"
      ],
      "metadata": {
        "id": "kpVzWRaBySsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arguments for loading model\n",
        "parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
        "# (model, data, training) arguments\n",
        "model_args, data_args, training_args = parser.parse_args_into_dataclasses(['--model_name_or_path', MODEL_NAME,\n",
        "                                                                               '--output_dir', OUTPUT_PATH, \n",
        "                                                                               '--data_path', DATA_PATH,\n",
        "                                                                               '--cache_dir', MODEL_PATH])\n",
        "    \n",
        "# pre-trained model\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    cache_dir=training_args.cache_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# pre-trained tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_args.model_name_or_path,\n",
        "    cache_dir=training_args.cache_dir,\n",
        "    model_max_length=training_args.model_max_length,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    smart_tokenizer_and_embedding_resize(\n",
        "        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "if \"llama\" in model_args.model_name_or_path:\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "        }\n",
        "    )\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "print(f'DONE: loading Model and Tokenizer - Llama')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326,
          "referenced_widgets": [
            "24b64daf669f40cf8b3f2e7f46cd7110",
            "ff04efe267de49a8ae83e84c2615fcfa",
            "78621d30272c4484839ffa2403d3e51c",
            "83960907b7434975b13da2efc4c42538",
            "17eb3ec77fa04144ac855135564774de",
            "58dd358714aa4befb92178fb25c80750",
            "f6753ae9cf884b158c0b0f6b7bc292a7",
            "184311c95054404bb5089c30f51ad94f",
            "80120daf176e4c8bb5c4f15c23a517f2",
            "9b5fa86bb8a843b592fd3f5b277eaee4",
            "c8b88b3ce0304460a2fb9a97d6456749"
          ]
        },
        "id": "VAruhZqL8zQf",
        "outputId": "713ce19b-692b-468f-8741-85e00fa850e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24b64daf669f40cf8b3f2e7f46cd7110"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Test Function\n"
      ],
      "metadata": {
        "id": "o4d7x9FV-lrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "    trainer.train()\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, \n",
        "                paired with an input that provides further context. \n",
        "                Write a response that appropriately completes the request.\n",
        "                ### Instruction:\n",
        "                {instruction}\n",
        "\n",
        "                ### Input:\n",
        "                {input}\n",
        "\n",
        "                ### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. \n",
        "                Write a response that appropriately completes the request.\n",
        "                ### Instruction:\n",
        "                {instruction}\n",
        "\n",
        "                ### Response:\"\"\"\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=256\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
      ],
      "metadata": {
        "id": "repYd0oG-oqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "QZvsc_-i_R0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #train()\n",
        "    evaluate(input(\"Instruction: \"))"
      ],
      "metadata": {
        "id": "y8uwPYor_Q0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Or3nnUF0xlTw",
        "B-CCfjCmxtSn",
        "AJz5MgGhyDYr",
        "TeOGOMaFyIj9",
        "kpVzWRaBySsG",
        "o4d7x9FV-lrf"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24b64daf669f40cf8b3f2e7f46cd7110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff04efe267de49a8ae83e84c2615fcfa",
              "IPY_MODEL_78621d30272c4484839ffa2403d3e51c",
              "IPY_MODEL_83960907b7434975b13da2efc4c42538"
            ],
            "layout": "IPY_MODEL_17eb3ec77fa04144ac855135564774de"
          }
        },
        "ff04efe267de49a8ae83e84c2615fcfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58dd358714aa4befb92178fb25c80750",
            "placeholder": "​",
            "style": "IPY_MODEL_f6753ae9cf884b158c0b0f6b7bc292a7",
            "value": "Loading checkpoint shards:  18%"
          }
        },
        "78621d30272c4484839ffa2403d3e51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_184311c95054404bb5089c30f51ad94f",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80120daf176e4c8bb5c4f15c23a517f2",
            "value": 6
          }
        },
        "83960907b7434975b13da2efc4c42538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b5fa86bb8a843b592fd3f5b277eaee4",
            "placeholder": "​",
            "style": "IPY_MODEL_c8b88b3ce0304460a2fb9a97d6456749",
            "value": " 6/33 [00:17&lt;01:09,  2.58s/it]"
          }
        },
        "17eb3ec77fa04144ac855135564774de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58dd358714aa4befb92178fb25c80750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6753ae9cf884b158c0b0f6b7bc292a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "184311c95054404bb5089c30f51ad94f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80120daf176e4c8bb5c4f15c23a517f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b5fa86bb8a843b592fd3f5b277eaee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8b88b3ce0304460a2fb9a97d6456749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}